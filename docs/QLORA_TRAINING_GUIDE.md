# QLoRA è®­ç»ƒæŒ‡å— - 32Bæ¨¡å‹8å¡å¹¶è¡Œ

## ğŸ“‹ æ¦‚è¿°

æ–°å¢çš„ `train_qlora.py` è„šæœ¬æ”¯æŒåœ¨8å¼ V100 (32GB)ä¸Šå¹¶è¡Œè®­ç»ƒQwen3-VL-32Bæ¨¡å‹ï¼Œä½¿ç”¨4-bité‡åŒ–èŠ‚çœæ˜¾å­˜ã€‚

**ç‰¹æ€§ï¼š**
- âœ… 4-bit NF4é‡åŒ–ï¼šæ¨¡å‹æ˜¾å­˜å ç”¨ ~18GB
- âœ… V100 FP16ä¼˜åŒ–ï¼šç¡¬ä»¶å…¼å®¹æ€§
- âœ… 8å¡æ•°æ®å¹¶è¡Œï¼šè®­ç»ƒé€Ÿåº¦å¿«
- âœ… LoRA rank=8ï¼šä½æ˜¾å­˜é«˜æ•ˆç‡
- âœ… æ™ºèƒ½å­¦ä¹ ç‡è°ƒåº¦ï¼šè‡ªåŠ¨é™ä½LR
- âœ… æ—©åœæœºåˆ¶ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
- âœ… å®Œå…¨ç‹¬ç«‹ï¼šä¸å½±å“åŸæœ‰ `train.py`

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. Forwardè®­ç»ƒï¼ˆæ··åˆretentionï¼‰

```bash
cd /work/mm_reversal_curse
source .venv/bin/activate

accelerate launch --num_processes=8 scripts/train_qlora.py \
  --config configs/config_qwen3vl32_fp16.yaml \
  --task forward \
  --name 4faces_qlora \
  --data_dir data/4faces \
  --face_retention_pool data/face_retention_pool \
  --retention_ratio 0.3
```

### 2. Reverseè®­ç»ƒ

```bash
accelerate launch --num_processes=8 scripts/train_qlora.py \
  --config configs/config_qwen3vl32_fp16.yaml \
  --task reverse \
  --name 4faces_qlora \
  --data_dir data/4faces
```

---

## âš™ï¸ é…ç½®æ–‡ä»¶è¯´æ˜

### config_qwen3vl32_fp16.yaml

```yaml
lora:
  r: 8                    # LoRA rank (é™ä½æ˜¾å­˜)
  alpha: 16               # LoRA alpha (2*rank)
  dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"

training:
  # === Learning Rate Configuration ===
  learning_rate: 1e-4              # åˆå§‹å­¦ä¹ ç‡
  min_lr: 6e-5                     # æœ€å°å­¦ä¹ ç‡é˜ˆå€¼
  lr_reduction_factor: 0.5         # LRè¡°å‡å› å­ï¼ˆæ¯æ¬¡å‡åŠï¼‰
  lr_patience: 1                   # LRè¡°å‡å‰ç­‰å¾…çš„epochæ•°
  improvement_threshold: 0.05      # Val lossæ”¹å–„é˜ˆå€¼ï¼ˆ5%ï¼‰
  min_val_loss: 0.2                # æ—©åœé˜ˆå€¼
  
  # === Batch Configuration ===
  batch_size: 1                    # æ¯å¡batch=1ï¼ˆå¿…é¡»ï¼‰
  gradient_accumulation_steps: 8   # æ¢¯åº¦ç´¯ç§¯ï¼ˆç­‰æ•ˆbatch=8*8=64ï¼‰
  num_epochs: 10
  max_length: 512
  warmup_ratio: 0.02
  weight_decay: 0.01
```

---

## ğŸ“Š æ˜¾å­˜å ç”¨åˆ†æ

### å•å¡æ˜¾å­˜ï¼ˆV100 32GBï¼‰

| ç»„ä»¶ | æ˜¾å­˜å ç”¨ |
|------|---------|
| åŸºç¡€æ¨¡å‹ (4-bit) | ~18GB |
| LoRAå‚æ•° (rank=8) | ~0.5GB |
| ä¼˜åŒ–å™¨çŠ¶æ€ | ~1GB |
| æ¢¯åº¦ + æ¿€æ´»å€¼ | ~8GB |
| **æ€»è®¡** | **~27.5GB** |

**å®‰å…¨ä½™é‡ï¼š** ~4.5GBï¼ˆè¶³å¤Ÿåº”å¯¹åŠ¨æ€æ³¢åŠ¨ï¼‰

---

## ğŸ¯ è®­ç»ƒç­–ç•¥

### å­¦ä¹ ç‡è°ƒåº¦

```
åˆå§‹LR: 1e-4
  â†“
Val lossä¸æ”¹å–„(patience=1 epoch)
  â†“
LR *= 0.5 â†’ 5e-5
  â†“
ç»§ç»­ä¸æ”¹å–„
  â†“
LR *= 0.5 â†’ 2.5e-5
  â†“
LR < min_lr (6e-5) â†’ åœæ­¢è®­ç»ƒ
```

### æ—©åœæ¡ä»¶

1. **Val lossé˜ˆå€¼ï¼š** `val_loss < 0.2` â†’ åœæ­¢
2. **LRé˜ˆå€¼ï¼š** `lr < 6e-5` â†’ åœæ­¢
3. **æ”¹å–„é˜ˆå€¼ï¼š** Val lossæ”¹å–„ < 5% â†’ ä¸ä¿å­˜ï¼Œç´¯ç§¯patience

---

## ğŸ“ è¾“å‡ºç»“æ„

```
outputs/4faces_qlora_forward/
â”œâ”€â”€ best/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.safetensors   # æœ€ä½³checkpointï¼ˆ153MBï¼‰
â”œâ”€â”€ final/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.safetensors   # æœ€ç»ˆcheckpoint
â””â”€â”€ training_history.json            # è®­ç»ƒå†å²
```

---

## ğŸ” è¯„ä¼°æ¨¡å‹

è®­ç»ƒå®Œæˆåä½¿ç”¨åŸæœ‰è¯„ä¼°è„šæœ¬ï¼š

```bash
python3 scripts/evaluate.py \
  --model_path outputs/4faces_qlora_forward/best \
  --base_model /work/models/qwen/Qwen3-VL-32B-Instruct \
  --data_dir data/4faces \
  --task all \
  --save_examples 10 \
  --device_map auto
```

---

## âš¡ æ€§èƒ½å¯¹æ¯”

| æ¨¡å¼ | å¡æ•° | æ˜¾å­˜/å¡ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|------|----------|
| **qlora** | 8 | ~28GB | å¿« | 32Bæ¨¡å‹æ¨è |
| auto | 3-4 | ~25GB | ä¸­ | å•ä»»åŠ¡æ¨ç† |
| deepspeed | 8 | OOM | - | 32Bä¸é€‚ç”¨ |

---

## ğŸ› ï¸ æ•…éšœæ’æŸ¥

### 1. OOMé”™è¯¯

**ç—‡çŠ¶ï¼š** `CUDA out of memory`

**è§£å†³ï¼š**
```bash
# æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨GPU
nvidia-smi

# é™ä½gradient_accumulation_steps
# config_qwen3vl32_fp16.yaml:
gradient_accumulation_steps: 4  # ä»8é™åˆ°4
```

### 2. Accelerateé…ç½®

**é¦–æ¬¡ä½¿ç”¨éœ€è¦é…ç½®ï¼š**
```bash
accelerate config

# é€‰æ‹©ï¼š
# - Compute environment: This machine
# - Distributed type: multi-GPU
# - Number of processes: 8
# - Mixed precision: fp16
```

### 3. è®­ç»ƒé€Ÿåº¦æ…¢

**æ£€æŸ¥ï¼š**
- Gradient Checkpointingå·²å¯ç”¨ï¼ˆä¼šç¨æ…¢ä½†èŠ‚çœæ˜¾å­˜ï¼‰
- Batch size=1æ˜¯å¿…é¡»çš„ï¼ˆ32Bæ¨¡å‹é™åˆ¶ï¼‰
- ç­‰æ•ˆbatché€šè¿‡gradient_accumulationå®ç°

---

## ğŸ“š æŠ€æœ¯ç»†èŠ‚

### QLoRAåŸç†

1. **4-bité‡åŒ–ï¼š** åŸºç¡€æ¨¡å‹å‹ç¼©åˆ°18GBï¼ˆåŸ64GBï¼‰
2. **LoRAè®­ç»ƒï¼š** åªè®­ç»ƒå°adapterï¼ˆFP16é«˜ç²¾åº¦ï¼‰
3. **æ··åˆç²¾åº¦ï¼š** å‰å‘ä¼ æ’­INT4ï¼Œæ¢¯åº¦è®¡ç®—FP16
4. **Paged Optimizerï¼š** ä¼˜åŒ–å™¨çŠ¶æ€å¯offloadåˆ°CPU

### å†»ç»“ç­–ç•¥

- âœ… Vision Encoder: å†»ç»“ï¼ˆä¸è®­ç»ƒï¼‰
- âœ… LLMéƒ¨åˆ†: LoRAå¾®è°ƒï¼ˆq/k/v/o projectionï¼‰
- âœ… æ€»å‚æ•°é‡: ~200Må¯è®­ç»ƒï¼ˆ0.6%ï¼‰

---

## ğŸ†š ä¸åŸæœ‰æ¨¡å¼å¯¹æ¯”

| ç‰¹æ€§ | train.py (auto) | train.py (deepspeed) | train_qlora.py |
|------|-----------------|---------------------|----------------|
| 32Bæ”¯æŒ | âœ… | âŒ (OOM) | âœ… |
| å¡æ•° | 3-4 | 8 | 8 |
| é‡åŒ– | æ—  | æ—  | 4-bit |
| å¹¶è¡Œæ–¹å¼ | æ¨¡å‹å¹¶è¡Œ | æ•°æ®å¹¶è¡Œ | æ•°æ®å¹¶è¡Œ |
| è®­ç»ƒé€Ÿåº¦ | æ…¢ | - | å¿« |
| æ˜¾å­˜/å¡ | ~25GB | OOM | ~28GB |
| **æ¨è** | å•æœºæ¨ç† | å°æ¨¡å‹ | **32Bè®­ç»ƒé¦–é€‰** |

---

## âœ… å®Œæ•´å·¥ä½œæµ

```bash
# 1. ç”Ÿæˆæ•°æ®
python scripts/generate_data.py --config configs/face_config.yaml

# 2. ç”Ÿæˆretention pool
python scripts/generate_face_retention_pool.py --num_entities 4

# 3. QLoRAè®­ç»ƒ
accelerate launch --num_processes=8 scripts/train_qlora.py \
  --config configs/config_qwen3vl32_fp16.yaml \
  --task forward \
  --name 4faces_qlora \
  --data_dir data/4faces \
  --face_retention_pool data/face_retention_pool \
  --retention_ratio 0.3

# 4. è¯„ä¼°
python scripts/evaluate.py \
  --model_path outputs/4faces_qlora_forward/best \
  --base_model /work/models/qwen/Qwen3-VL-32B-Instruct \
  --data_dir data/4faces \
  --task all \
  --device_map auto
```

---

**æ³¨æ„ï¼š** `train_qlora.py` å®Œå…¨ç‹¬ç«‹ï¼Œä¸å½±å“åŸæœ‰ `train.py` çš„ä»»ä½•åŠŸèƒ½ã€‚åŸæœ‰çš„ `--mode auto` å’Œ `--mode deepspeed` ä»ç„¶å¯ä»¥æ­£å¸¸ä½¿ç”¨ã€‚
