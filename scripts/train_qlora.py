#!/usr/bin/env python3
"""
==============================================================================
QLoRA è®­ç»ƒè„šæœ¬ - 32B æ¨¡å‹ 8å¡å¹¶è¡Œè®­ç»ƒï¼ˆV100ä¸“ç”¨ï¼‰
==============================================================================

åŠŸèƒ½ï¼š
  - ä½¿ç”¨ 4-bit é‡åŒ– (NF4) åŠ è½½ 32B æ¨¡å‹
  - 8å¼ V100å¹¶è¡Œæ•°æ®å¹¶è¡Œè®­ç»ƒ (Accelerate)
  - æ¯å¼ å¡æ˜¾å­˜å ç”¨ï¼š~28GBï¼ˆæ¨¡å‹18GB + è®­ç»ƒ10GBï¼‰
  - LoRA rank=8ï¼Œä»…è®­ç»ƒ LLM éƒ¨åˆ†ï¼Œå†»ç»“ Vision Encoder
  - æ”¯æŒ Forward/Reverse ä»»åŠ¡

==============================================================================
ä½¿ç”¨æ–¹æ³•
==============================================================================

# Forward è®­ç»ƒ (æ··åˆretention)
accelerate launch --num_processes=8 scripts/train_qlora.py \
    --config configs/config_qwen3vl32_fp16.yaml \
    --task forward \
    --name test_20_qlora \
    --data_dir data/test_20_r32 \
    --face_retention_pool data/face_retention_pool \
    --retention_ratio 0.3

# Reverse è®­ç»ƒ
accelerate launch --num_processes=8 scripts/train_qlora.py \
    --config configs/config_qwen3vl32_fp16.yaml \
    --task reverse \
    --name test_20_qlora \
    --data_dir data/test_20_r32

==============================================================================
"""
import warnings
import os
import sys
import json
import yaml
import argparse
from pathlib import Path

import torch
from transformers import (
    AutoProcessor, 
    AutoModelForImageTextToText, 
    BitsAndBytesConfig,
    get_constant_schedule
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from torch.utils.data import DataLoader
from torch.optim import AdamW
try:
    import bitsandbytes as bnb
    HAS_BNB = True
except ImportError:
    HAS_BNB = False
from tqdm import tqdm

# Accelerate for multi-GPU data parallel
try:
    from accelerate import Accelerator
    from accelerate.utils import DistributedDataParallelKwargs
except ImportError:
    raise ImportError("è¯·å®‰è£… accelerate: pip install accelerate")

warnings.filterwarnings("ignore", message=".*torch_dtype.*")
os.environ.setdefault('TORCH_CPP_LOG_LEVEL', 'ERROR')

sys.path.insert(0, str(Path(__file__).parent.parent))
from src.data.dataset import MixedForwardDataset, collate_fn


def setup_model_qlora(config: dict, accelerator: Accelerator):
    """
    è®¾ç½® QLoRA æ¨¡å‹ (4-bité‡åŒ– + LoRA)
    
    ç‰¹æ€§:
    - 4-bit NF4é‡åŒ–ï¼Œæ˜¾å­˜å ç”¨ ~18GB
    - FP16è®¡ç®—ï¼ˆV100ä¸æ”¯æŒBF16ï¼‰
    - åŒé‡é‡åŒ–èŠ‚çœé¢å¤–æ˜¾å­˜
    - å†»ç»“ Vision Encoder
    - LoRA rank=8 (ä½æ˜¾å­˜)
    """
    model_path = config["model"]["name_or_path"]
    
    accelerator.print(f"Loading model with QLoRA: {model_path}")
    accelerator.print("  - 4-bit quantization (NF4)")
    accelerator.print("  - FP16 compute (V100 compatible)")
    accelerator.print("  - Double quantization enabled")
    
    # === 1. é‡åŒ–é…ç½® (QLoRAæ ¸å¿ƒ) ===
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # === 2. åŠ è½½åŸºç¡€æ¨¡å‹ ===
    model = AutoModelForImageTextToText.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map={"": accelerator.process_index},
        torch_dtype=torch.float16,
        trust_remote_code=True,
        attn_implementation="eager"
    )
    
    # === 3. å‡†å¤‡æ¨¡å‹ç”¨äºkbitè®­ç»ƒ ===
    model = prepare_model_for_kbit_training(model)
    model.gradient_checkpointing_enable()
    
    # === 4. å†»ç»“ Vision Encoder ===
    if hasattr(model, 'visual'):
        for param in model.visual.parameters():
            param.requires_grad = False
        accelerator.print("  - Vision Encoder frozen")
    
    # === 5. LoRAé…ç½® ===
    lora_config = LoraConfig(
        r=int(config["lora"]["r"]),
        lora_alpha=int(config["lora"]["alpha"]),
        lora_dropout=float(config["lora"]["dropout"]),
        target_modules=config["lora"]["target_modules"],
        task_type=TaskType.CAUSAL_LM,
        bias="none"
    )
    
    # === 6. åº”ç”¨ LoRA ===
    model = get_peft_model(model, lora_config)
    accelerator.print("\n=== Trainable Parameters ===")
    model.print_trainable_parameters()
    
    # === 7. åŠ è½½ Processorï¼ˆé™åˆ¶å›¾åƒåˆ†è¾¨ç‡ï¼‰===
    max_pixels = int(config["training"].get("max_pixels", 262144))  # 512*512
    min_pixels = int(config["training"].get("min_pixels", 3136))    # 56*56
    
    processor = AutoProcessor.from_pretrained(
        model_path, 
        trust_remote_code=True,
        max_pixels=max_pixels,
        min_pixels=min_pixels
    )
    accelerator.print(f"  - Image resolution: {min_pixels} ~ {max_pixels} pixels")
    
    return model, processor


def train_qlora(args, config):
    """
    QLoRA å¤šå¡å¹¶è¡Œè®­ç»ƒï¼ˆä½¿ç”¨ Accelerateï¼‰
    
    æµç¨‹:
    1. åˆå§‹åŒ– Accelerate
    2. è®¾ç½®æ¨¡å‹ï¼ˆ4-bité‡åŒ– + LoRAï¼‰
    3. å‡†å¤‡æ•°æ®é›†å’Œä¼˜åŒ–å™¨
    4. ä½¿ç”¨ accelerator.prepare() åŒ…è£…
    5. è®­ç»ƒå¾ªç¯
    """
    # === 1. åˆå§‹åŒ– Accelerator ===
    kwargs = DistributedDataParallelKwargs(find_unused_parameters=False)
    accelerator = Accelerator(
        gradient_accumulation_steps=int(config["training"]["gradient_accumulation_steps"]),
        mixed_precision="fp16",
        kwargs_handlers=[kwargs]
    )
    
    task = args.task
    retention_ratio = args.retention_ratio
    
    accelerator.print(f"\n{'='*70}")
    accelerator.print(f"QLoRA Training: {task.upper()} Task")
    accelerator.print(f"  - GPUs: {accelerator.num_processes}")
    accelerator.print(f"  - Mixed Precision: FP16 (V100 compatible)")
    accelerator.print(f"  - Gradient Accumulation Steps: {accelerator.gradient_accumulation_steps}")
    if task == "forward":
        accelerator.print(f"  - Retention Ratio: {retention_ratio}")
    accelerator.print(f"{'='*70}\n")
    
    # === 2. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ ===
    model, processor = setup_model_qlora(config, accelerator)
    
    # === 3. å‡†å¤‡æ•°æ®é›† ===
    data_dir = Path(args.data_dir) if args.data_dir else Path(config["data"]["output_dir"])
    max_length = int(config["training"].get("max_length", 512))
    
    if task == "forward":
        train_file = data_dir / "forward_train.jsonl"
        val_file = data_dir / "forward_val.jsonl"
        train_dataset = MixedForwardDataset(
            str(train_file), processor, max_length, 
            retention_ratio=retention_ratio, seed=42,
            split="train", 
            retention_pool_dir=args.retention_pool,
            face_retention_pool_dir=args.face_retention_pool, 
            face_retention_ratio=args.face_retention_ratio
        )
        val_dataset = MixedForwardDataset(
            str(val_file), processor, max_length, 
            retention_ratio=retention_ratio, seed=42,
            split="val", 
            retention_pool_dir=args.retention_pool,
            face_retention_pool_dir=args.face_retention_pool, 
            face_retention_ratio=args.face_retention_ratio
        )
    else:
        from src.data.dataset import ReverseDataset
        train_file = data_dir / "reverse_train.jsonl"
        val_file = data_dir / "reverse_val.jsonl"
        train_dataset = ReverseDataset(str(train_file), processor, max_length)
        val_dataset = ReverseDataset(str(val_file), processor, max_length)
    
    accelerator.print(f"Dataset loaded:")
    accelerator.print(f"  - Train samples: {len(train_dataset)}")
    accelerator.print(f"  - Val samples: {len(val_dataset)}\n")
    
    # === 4. è®­ç»ƒå‚æ•° ===
    batch_size = int(config["training"].get("batch_size", 
                     config["training"].get("per_device_train_batch_size", 1)))
    grad_accum = int(config["training"]["gradient_accumulation_steps"])
    num_epochs = int(config["training"]["num_epochs"])
    learning_rate = float(config["training"]["learning_rate"])
    weight_decay = float(config["training"].get("weight_decay", 0.01))
    warmup_ratio = float(config["training"].get("warmup_ratio", 0.05))
    
    # === 5. DataLoader ===
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_fn, 
        num_workers=0,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=collate_fn, 
        num_workers=0,
        pin_memory=True
    )
    
    accelerator.print(f"Training config:")
    accelerator.print(f"  - Batch size per device: {batch_size}")
    accelerator.print(f"  - Gradient accumulation: {grad_accum}")
    accelerator.print(f"  - Effective batch size: {batch_size * accelerator.num_processes * grad_accum}")
    accelerator.print(f"  - Learning rate: {learning_rate}")
    accelerator.print(f"  - Epochs: {num_epochs}")
    # warmup_steps å°†åœ¨åé¢è®¡ç®—åæ‰“å°
    
    # === 6. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ===
    use_8bit_optim = config["training"].get("use_8bit_optimizer", False)
    
    if use_8bit_optim and HAS_BNB:
        accelerator.print("  - Using 8-bit AdamW optimizer (saves ~2GB VRAM)")
        optimizer = bnb.optim.AdamW8bit(
            [p for p in model.parameters() if p.requires_grad],
            lr=learning_rate,
            weight_decay=weight_decay
        )
    else:
        optimizer = AdamW(
            [p for p in model.parameters() if p.requires_grad],
            lr=learning_rate,
            weight_decay=weight_decay
        )
    
    # === 7. Accelerate åŒ…è£…ï¼ˆå…ˆprepareï¼Œå†åˆ›å»ºschedulerï¼‰===
    model, optimizer, train_loader, val_loader = accelerator.prepare(
        model, optimizer, train_loader, val_loader
    )
    
    # åœ¨ prepare ä¹‹åè®¡ç®— training stepsï¼ˆtrain_loader å·²è¢«åˆ†ç‰‡åˆ°æ¯ä¸ªè¿›ç¨‹ï¼‰
    steps_per_epoch = len(train_loader) // grad_accum
    num_training_steps = steps_per_epoch * num_epochs
    warmup_steps = 0  # ç¦ç”¨warmupï¼Œç›´æ¥ä½¿ç”¨ç›®æ ‡å­¦ä¹ ç‡
    # warmup_steps = max(warmup_steps, 1)  # å·²ç¦ç”¨warmup
    
    accelerator.print(f"  - Steps per epoch (per GPU): {steps_per_epoch}")
    accelerator.print(f"  - Total training steps: {num_training_steps}")
    accelerator.print(f"  - Warmup steps: {warmup_steps} ({warmup_ratio*100:.1f}%)")
    
    # åœ¨ prepare ä¹‹ååˆ›å»º schedulerï¼Œä½¿ç”¨è¢«åŒ…è£…åçš„ optimizer
    scheduler = get_constant_schedule(
        optimizer,
        
        
    )
    
    # === 8. è¾“å‡ºç›®å½• ===
    output_dir = Path("outputs") / f"{args.name}_{task}" if args.name else \
                 Path(config["training"]["output_dir"]) / f"{task}_trained"
    if accelerator.is_main_process:
        output_dir.mkdir(parents=True, exist_ok=True)
    
    # === 9. è®­ç»ƒå†å² ===
    history = {
        "task": task,
        "mode": "qlora_accelerate",
        "num_gpus": accelerator.num_processes,
        "train_loss": [],
        "val_loss": [],
        "epochs": [],
        "step_logs": []  # è®°å½•æ¯ä¸ªstepçš„è¯¦ç»†ä¿¡æ¯
    }
    best_val_loss = float('inf')
    global_step = 0  # å…¨å±€stepè®¡æ•°å™¨
    
    # === 10. è®­ç»ƒå¾ªç¯ ===
    accelerator.print(f"{'='*70}")
    accelerator.print("Starting training...")
    accelerator.print(f"{'='*70}\n")
    
    for epoch in range(num_epochs):
        # --- Training ---
        model.train()
        epoch_loss = 0.0
        num_batches = 0
        
        progress = tqdm(
            train_loader, 
            desc=f"Epoch {epoch+1}/{num_epochs}",
            disable=not accelerator.is_local_main_process
        )
        
        for batch in progress:
            with accelerator.accumulate(model):
                # ç§»é™¤ä»»åŠ¡ç±»å‹æ ‡è®°
                batch.pop("task_type", None)
                
                # å‰å‘ä¼ æ’­
                outputs = model(**batch)
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                
                # æ¢¯åº¦è£å‰ª
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(model.parameters(), 1.0)
                
                # æ›´æ–°å‚æ•°
                optimizer.step()
                optimizer.zero_grad()
                
                # scheduleråªåœ¨çœŸæ­£æ›´æ–°å‚æ•°æ—¶step
                if accelerator.sync_gradients:
                    scheduler.step()
            
            # è®°å½•æŸå¤±
            epoch_loss += loss.item()
            num_batches += 1
            
            # åªåœ¨çœŸæ­£æ›´æ–°å‚æ•°æ—¶è®°å½•stepä¿¡æ¯
            if accelerator.sync_gradients:
                global_step += 1
                current_lr = optimizer.param_groups[0]['lr']
                
                # è®°å½•stepçº§åˆ«çš„log
                if accelerator.is_main_process:
                    step_log = {
                        "global_step": global_step,
                        "epoch": epoch + 1,
                        "loss": loss.item(),
                        "lr": current_lr
                    }
                    history["step_logs"].append(step_log)
            
            # æ›´æ–°è¿›åº¦æ¡
            if accelerator.is_local_main_process:
                current_lr = optimizer.param_groups[0]['lr']
                progress.set_postfix({
                    "loss": f"{loss.item():.4f}",
                    "lr": f"{current_lr:.2e}"
                })
        
        avg_train_loss = epoch_loss / num_batches
        
        # --- Validation ---
        model.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                batch.pop("task_type", None)
                outputs = model(**batch)
                val_loss += outputs.loss.item()
                val_batches += 1
        
        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0.0
        
        # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æŸå¤±
        avg_train_loss = accelerator.gather(torch.tensor([avg_train_loss]).to(accelerator.device)).mean().item()
        avg_val_loss = accelerator.gather(torch.tensor([avg_val_loss]).to(accelerator.device)).mean().item()
        
        # --- è®°å½•å’Œä¿å­˜ ---
        history["train_loss"].append(avg_train_loss)
        history["val_loss"].append(avg_val_loss)
        history["epochs"].append(epoch + 1)
        
        if accelerator.is_main_process:
            accelerator.print(f"\nEpoch {epoch+1}/{num_epochs}:")
            accelerator.print(f"  - Train Loss: {avg_train_loss:.4f}")
            accelerator.print(f"  - Val Loss: {avg_val_loss:.4f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_dir = output_dir / "best"
                best_dir.mkdir(exist_ok=True)
                
                # è§£åŒ…æ¨¡å‹å†ä¿å­˜
                unwrapped_model = accelerator.unwrap_model(model)
                unwrapped_model.save_pretrained(str(best_dir))
                
                accelerator.print(f"  ğŸ’¾ Saved best model (val_loss={best_val_loss:.4f})")
            
            accelerator.print("")
        
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹
        accelerator.wait_for_everyone()
    
    # === 11. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ===
    if accelerator.is_main_process:
        final_dir = output_dir / "final"
        final_dir.mkdir(exist_ok=True)
        
        unwrapped_model = accelerator.unwrap_model(model)
        unwrapped_model.save_pretrained(str(final_dir))
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(output_dir / "training_history.json", "w") as f:
            json.dump(history, f, indent=2)
        
        accelerator.print(f"\n{'='*70}")
        accelerator.print("âœ“ Training Complete!")
        accelerator.print(f"  - Output: {output_dir}")
        accelerator.print(f"  - Best Val Loss: {best_val_loss:.4f}")
        accelerator.print(f"{'='*70}\n")


def main():
    """ä¸»å‡½æ•°ï¼šè§£æå‚æ•°å¹¶å¯åŠ¨è®­ç»ƒ"""
    parser = argparse.ArgumentParser(description="QLoRA è®­ç»ƒè„šæœ¬ï¼ˆ8å¡V100å¹¶è¡Œï¼‰")
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--config", type=str, required=True,
                       help="é…ç½®æ–‡ä»¶è·¯å¾„ (yaml)")
    parser.add_argument("--task", type=str, required=True, 
                       choices=["forward", "reverse"],
                       help="ä»»åŠ¡ç±»å‹: forward / reverse")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--name", type=str, default=None,
                       help="å®éªŒåç§°ï¼Œè¾“å‡ºåˆ° outputs/<name>_<task>/")
    parser.add_argument("--data_dir", type=str, default=None,
                       help="æ•°æ®ç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨configä¸­çš„å€¼ï¼‰")
    parser.add_argument("--retention_ratio", type=float, default=0.3,
                       help="Forwardä»»åŠ¡çš„retentionæ¯”ä¾‹")
    parser.add_argument("--face_retention_pool", type=str, default="data/face_retention_pool",
                       help="äººè„¸retentionæ± ç›®å½•")
    parser.add_argument("--retention_pool", type=str, default=None,
                       help="ç‰©ä½“retentionæ± ç›®å½•")
    parser.add_argument("--face_retention_ratio", type=float, default=1.0,
                       help="äººè„¸æ± å retentionçš„æ¯”ä¾‹")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config) as f:
        config = yaml.safe_load(f)
    
    # å¯åŠ¨è®­ç»ƒ
    train_qlora(args, config)


if __name__ == "__main__":
    main()
