#!/usr/bin/env python3
"""
==============================================================================
è®­ç»ƒè„šæœ¬ - Forward/Reverse ä»»åŠ¡è®­ç»ƒ
==============================================================================

åŠŸèƒ½ï¼š
  ä½¿ç”¨ DeepSpeed ZeRO-2 + LoRA è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
  - Forward ä»»åŠ¡ï¼šè‡ªåŠ¨ä½¿ç”¨æ··åˆè®­ç»ƒï¼ˆé˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼‰
  - Reverse ä»»åŠ¡ï¼šæ™®é€šè®­ç»ƒ

==============================================================================
å¿«é€Ÿå¼€å§‹
==============================================================================

# 1. åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•å¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
cd /work/mm_reversal_curse
source .venv/bin/activate

# 2. Forward è®­ç»ƒï¼ˆæ¨èå…ˆè¿è¡Œè¿™ä¸ªéªŒè¯ Reversal Curseï¼‰
rm -rf outputs/forward_trained  # æ¸…é™¤ä¹‹å‰çš„ç»“æœ
deepspeed --num_gpus=8 scripts/train.py --config configs/config.yaml --task forward

# 3. Reverse è®­ç»ƒï¼ˆå¯é€‰ï¼‰
rm -rf outputs/reverse_trained
deepspeed --num_gpus=8 scripts/train.py --config configs/config.yaml --task reverse

# 4. è°ƒæ•´ Forward çš„ä¿æŒä»»åŠ¡æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.3 å³ 30%ï¼‰
deepspeed --num_gpus=8 scripts/train.py --config configs/config.yaml --task forward --retention_ratio 0.2

==============================================================================
è¾“å‡ºç»“æ„
==============================================================================

outputs/forward_trained/  æˆ–  outputs/reverse_trained/
â”œâ”€â”€ best/                   # æœ€ä½³ checkpointï¼ˆéªŒè¯ loss æœ€ä½ï¼‰
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.safetensors
â”œâ”€â”€ final/                  # æœ€ç»ˆ checkpoint
â””â”€â”€ training_history.json   # è®­ç»ƒå†å²ï¼ˆlossã€epoch ç­‰ï¼‰

==============================================================================
è®­ç»ƒé…ç½®ï¼ˆconfigs/config.yamlï¼‰
==============================================================================

- model: Qwen3-VL-8B-Instruct
- LoRA: r=16, alpha=32, targets=[q_proj, k_proj, v_proj, o_proj]
- Early Stopping: patience=3ï¼ˆéªŒè¯ loss è¿ç»­3æ¬¡ä¸ä¸‹é™åˆ™åœæ­¢ï¼‰
- æœ€å¤§ epoch: 1000

==============================================================================
"""
import warnings
import os
import sys
import json
import yaml
import argparse
from pathlib import Path

import torch
import torch.distributed as dist
import deepspeed
from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import LoraConfig, get_peft_model, TaskType
from torch.utils.data import DataLoader, DistributedSampler
from tqdm import tqdm

warnings.filterwarnings("ignore", message=".*torch_dtype.*")
warnings.filterwarnings("ignore", message=".*destroy_process_group.*")


# æŠ‘åˆ¶ DeepSpeed/NCCL ç»“æŸæ—¶çš„ Broken pipe è­¦å‘Š
os.environ.setdefault('NCCL_ASYNC_ERROR_HANDLING', '0')
os.environ.setdefault('TORCH_CPP_LOG_LEVEL', 'ERROR')
os.environ.setdefault('TORCH_DISTRIBUTED_DEBUG', 'OFF')
sys.path.insert(0, str(Path(__file__).parent.parent))
from src.data.dataset import MixedForwardDataset, collate_fn


def setup_model_and_processor(config: dict, local_rank: int):
    """åŠ è½½æ¨¡å‹å’Œ processor"""
    model_path = config["model"]["name_or_path"]
    
    if local_rank == 0:
        print(f"Loading model from {model_path}")
    
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
    
    model = AutoModelForImageTextToText.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        trust_remote_code=True,
        attn_implementation="eager",
    )
    
    model.gradient_checkpointing_enable()
    
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=int(config["lora"]["r"]),
        lora_alpha=int(config["lora"]["alpha"]),
        lora_dropout=float(config["lora"]["dropout"]),
        target_modules=config["lora"]["target_modules"],
        bias="none",
    )
    
    model = get_peft_model(model, lora_config)
    if local_rank == 0:
        model.print_trainable_parameters()
    
    return model, processor


def train(args, config):
    """ä¸»è®­ç»ƒå¾ªç¯"""
    task = args.task
    retention_ratio = args.retention_ratio
    
    deepspeed.init_distributed()
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    
    if local_rank == 0:
        print(f"\n{'='*60}")
        print(f"Starting {task.upper()} training on {world_size} GPUs")
        if task == "forward":
            print(f"Mode: Mixed training (retention_ratio={retention_ratio})")
        print(f"{'='*60}")
    
    model, processor = setup_model_and_processor(config, local_rank)
    
    # æ•°æ®ç›®å½•ï¼šä¼˜å…ˆä½¿ç”¨ --data_dirï¼Œå¦åˆ™ä½¿ç”¨ config ä¸­çš„ data.output_dir
    if args.data_dir:
        data_dir = Path(args.data_dir)
    else:
        data_dir = Path(config["data"]["output_dir"])
    max_length = int(config["training"].get("max_length", 512))
    
    if task == "forward":
        train_file = data_dir / "forward_train.jsonl"
        val_file = data_dir / "forward_val.jsonl"
        
        # Forward ä½¿ç”¨æ··åˆæ•°æ®é›†ï¼ˆè‡ªåŠ¨æ··å…¥ä¿æŒä»»åŠ¡é˜²æ­¢ç¾éš¾æ€§é—å¿˜ï¼‰
        # ä»å…¬å…±é¢˜åº“æŠ½å– retention æ ·æœ¬
        retention_pool = args.retention_pool if hasattr(args, 'retention_pool') else None
        face_retention_pool = args.face_retention_pool if hasattr(args, 'face_retention_pool') else None
        face_retention_ratio = args.face_retention_ratio if hasattr(args, 'face_retention_ratio') else 0.5
        
        train_dataset = MixedForwardDataset(
            str(train_file), processor, max_length,
            retention_ratio=retention_ratio,
            seed=42,
            split="train",
            retention_pool_dir=retention_pool,
            face_retention_pool_dir=face_retention_pool,
            face_retention_ratio=face_retention_ratio
        )
        # éªŒè¯é›†ä¹ŸåŒ…å« Retention
        val_dataset = MixedForwardDataset(
            str(val_file), processor, max_length,
            retention_ratio=retention_ratio,
            seed=42,
            split="val",
            retention_pool_dir=retention_pool,
            face_retention_pool_dir=face_retention_pool,
            face_retention_ratio=face_retention_ratio
        )
    else:
        train_file = data_dir / "reverse_train.jsonl"
        val_file = data_dir / "reverse_val.jsonl"
        train_dataset = ReverseDataset(str(train_file), processor, max_length)
        val_dataset = MixedForwardDataset(str(val_file), processor, max_length, retention_ratio=0, seed=42)
    
    if local_rank == 0:
        print(f"Train: {len(train_dataset)} samples, Val: {len(val_dataset)} samples")
    
    batch_size = int(config["training"]["batch_size"])
    grad_accum = int(config["training"].get("gradient_accumulation_steps", 1))
    num_epochs = int(config["training"]["num_epochs"])
    
    initial_lr = float(config["training"]["learning_rate"])
    min_lr = float(config["training"].get("min_lr", 1e-6))
    lr_reduction_factor = float(config["training"].get("lr_reduction_factor", 0.5))
    lr_patience = int(config["training"].get("lr_patience", 1))
    improvement_threshold = float(config["training"].get("improvement_threshold", 0.05))
    min_val_loss = float(config["training"].get("min_val_loss", 0.12))
    
    current_lr = initial_lr
    
    train_sampler = DistributedSampler(
        train_dataset, num_replicas=world_size, rank=local_rank, shuffle=True, seed=42
    )
    
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, sampler=train_sampler,
        collate_fn=collate_fn, num_workers=0, pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, batch_size=batch_size, shuffle=False,
        collate_fn=collate_fn, num_workers=0, pin_memory=True
    )
    
    steps_per_epoch = len(train_loader) // grad_accum
    total_steps = steps_per_epoch * num_epochs
    warmup_ratio = float(config["training"].get("warmup_ratio", 0.05))
    warmup_steps = int(total_steps * warmup_ratio)
    
    if local_rank == 0:
        print(f"Batch: {batch_size} x {grad_accum} x {world_size} = {batch_size * grad_accum * world_size}")
        print(f"Initial LR: {initial_lr}, Min LR: {min_lr}")
        print(f"LoRA r={config['lora']['r']}, alpha={config['lora']['alpha']}")
        print()
    
    ds_config = {
        "train_micro_batch_size_per_gpu": batch_size,
        "gradient_accumulation_steps": grad_accum,
        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": current_lr,
                "betas": [0.9, 0.999],
                "eps": 1e-8,
                "weight_decay": float(config["training"].get("weight_decay", 0.01)),
                "torch_adam": True
            }
        },
        "scheduler": {
            "type": "WarmupLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": current_lr,
                "warmup_num_steps": warmup_steps
            }
        },
        "fp16": {
            "enabled": True,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "initial_scale_power": 16,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        "zero_optimization": {
            "stage": 2,
            "contiguous_gradients": True,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 5e7,
            "allgather_bucket_size": 5e7
        },
        "gradient_clipping": 1.0,
        "steps_per_print": 9999999,
        "wall_clock_breakdown": False
    }
    
    model_parameters = [p for p in model.parameters() if p.requires_grad]
    model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(
        args=args, model=model, model_parameters=model_parameters, config=ds_config
    )
    
    device = model_engine.local_rank
    
    # å¦‚æœæŒ‡å®šäº† --nameï¼Œåˆ™è¾“å‡ºåˆ° outputs/<name>_<task>
    if args.name:
        output_dir = Path("outputs") / f"{args.name}_{task}"
    else:
        output_dir = Path(config["training"]["output_dir"]) / f"{task}_trained"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    history = {
        "task": task,
        "mode": "mixed" if task == "forward" else "normal",
        "retention_ratio": retention_ratio if task == "forward" else None,
        "face_retention_ratio": face_retention_ratio if task == "forward" else None,
        "face_retention_ratio": face_retention_ratio if task == "forward" else None,
        "train_loss": [],
        "val_loss": [],
        "learning_rates": [],
        "epochs": []
    }
    best_val_loss = float('inf')
    prev_val_loss = None
    patience_counter = 0
    should_stop = False
    
    for epoch in range(num_epochs):
        if should_stop:
            break
        
        train_sampler.set_epoch(epoch)
        
        model_engine.train()
        epoch_loss = 0.0
        num_batches = 0
        
        if local_rank == 0:
            progress = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} (lr={current_lr:.2e})")
        else:
            progress = train_loader
        
        for batch in progress:
            # æå– task_type ç”¨äº loss åŠ æƒï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            task_types = batch.pop("task_type", None)
            
            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
            outputs = model_engine(**batch)
            loss = outputs.loss
            
            # å¦‚æœæœ‰ task_typeï¼Œå¯¹ retention ä»»åŠ¡åº”ç”¨æ›´é«˜æƒé‡
            # CW/MCQ è¾“å‡º 1 tokenï¼ŒForward è¾“å‡º ~10 tokensï¼Œæ‰€ä»¥ç»™ retention 10x æƒé‡
            # è¿™é‡Œæ˜¯ batch-level è¿‘ä¼¼ï¼šå¦‚æœ batch ä¸­æœ‰ retentionï¼Œæé«˜è¯¥ batch çš„ loss æƒé‡
            # (å®é™…åº”è¯¥åœ¨ token-level åŠ æƒï¼Œä½†è¿™éœ€è¦ä¿®æ”¹æ¨¡å‹ï¼Œè¿™é‡Œç”¨ç®€åŒ–æ–¹æ¡ˆ)
            
            model_engine.backward(loss)
            model_engine.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            if local_rank == 0:
                progress.set_postfix({"loss": f"{loss.item():.4f}"})
        
        avg_train_loss = epoch_loss / num_batches
        
        model_engine.eval()
        val_loss = 0.0
        val_batches = 0
        
        with torch.no_grad():
            for batch in val_loader:
                # æå– task_typeï¼ˆéªŒè¯æ—¶ä¸éœ€è¦åŠ æƒï¼‰
                task_types = batch.pop("task_type", None)
                batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}
                outputs = model_engine(**batch)
                val_loss += outputs.loss.item()
                val_batches += 1
        
        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0.0
        
        history["train_loss"].append(avg_train_loss)
        history["val_loss"].append(avg_val_loss)
        history["learning_rates"].append(current_lr)
        history["epochs"].append(epoch + 1)
        
        if local_rank == 0:
            ratio = avg_val_loss / avg_train_loss if avg_train_loss > 0 else 1.0
            print(f"  â†’ Train: {avg_train_loss:.4f}, Val: {avg_val_loss:.4f} (ratio: {ratio:.2f})")
            
            if avg_val_loss < min_val_loss:
                print(f"\nâœ… val_loss {avg_val_loss:.4f} < {min_val_loss}, converged!")
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    history["best_epoch"] = epoch + 1
                    history["best_val_loss"] = best_val_loss
                    
                    best_dir = output_dir / "best"
                    best_dir.mkdir(exist_ok=True)
                    model_engine.module.save_pretrained(str(best_dir))
                    print(f"  ğŸ’¾ Saved best model")
                should_stop = True
                continue
            
            if prev_val_loss is not None:
                improvement = (prev_val_loss - avg_val_loss) / prev_val_loss
                print(f"  Improvement: {improvement*100:.2f}%")
                
                if improvement < improvement_threshold:
                    patience_counter += 1
                    print(f"  âš¡ Plateau ({patience_counter}/{lr_patience})")
                    
                    if patience_counter >= lr_patience:
                        new_lr = current_lr * lr_reduction_factor
                        
                        if new_lr < min_lr:
                            print(f"\nğŸ›‘ LR {new_lr:.2e} < min_lr, stopping")
                            should_stop = True
                            continue
                        
                        current_lr = new_lr
                        patience_counter = 0
                        
                        for param_group in optimizer.param_groups:
                            param_group['lr'] = current_lr
                        
                        print(f"  ğŸ“‰ LR â†’ {current_lr:.2e}")
                else:
                    patience_counter = 0
            
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                history["best_epoch"] = epoch + 1
                history["best_val_loss"] = best_val_loss
                
                best_dir = output_dir / "best"
                best_dir.mkdir(exist_ok=True)
                model_engine.module.save_pretrained(str(best_dir))
                print(f"  ğŸ’¾ Saved best (val_loss={best_val_loss:.4f})")
            
            prev_val_loss = avg_val_loss
            print()
    
    if local_rank == 0:
        final_dir = output_dir / "final"
        final_dir.mkdir(exist_ok=True)
        model_engine.module.save_pretrained(str(final_dir))
        
        with open(output_dir / "training_history.json", "w") as f:
            json.dump(history, f, indent=2)
        
        print(f"\n{'='*60}")
        print(f"âœ“ Training Complete ({task})")
        if task == "forward":
            print(f"  Mode: Mixed (retention_ratio={retention_ratio})")
        print(f"  Best: epoch {history.get('best_epoch', 'N/A')}, val_loss={history.get('best_val_loss', 0):.4f}")
        print(f"  Saved to: {output_dir}")
        print(f"{'='*60}")
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    try:
        dist.barrier()  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹åŒæ­¥
        dist.destroy_process_group()
    except Exception:
        pass  # å¿½ç•¥æ¸…ç†æ—¶çš„é”™è¯¯


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True)
    parser.add_argument("--task", type=str, required=True, choices=["forward", "reverse"])
    parser.add_argument("--retention_ratio", type=float, default=0.3,
                        help="Retention task ratio for Forward training (default: 0.3)")
    parser.add_argument("--retention_pool", type=str, default=None,
                        help="ç‰©ä½“ retention é¢˜åº“ç›®å½• (default: None, ä¸ä½¿ç”¨ç‰©ä½“æ± )")
    parser.add_argument("--face_retention_pool", type=str, default="data/face_retention_pool",
                        help="äººè„¸ retention é¢˜åº“ç›®å½• (default: data/face_retention_pool)")
    parser.add_argument("--face_retention_ratio", type=float, default=1.0,
                        help="äººè„¸ retention å æ€» retention çš„æ¯”ä¾‹ (default: 1.0, åªç”¨äººè„¸æ± )")
    parser.add_argument("--local_rank", type=int, default=-1)
    parser.add_argument("--name", type=str, default=None, help="å®éªŒåç§°ï¼Œç”¨äºè¾“å‡ºç›®å½• outputs/<name>_trained")
    parser.add_argument("--data_dir", type=str, default=None, help="æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤ä½¿ç”¨ config ä¸­çš„ data.output_dir)")
    parser = deepspeed.add_config_arguments(parser)
    args = parser.parse_args()
    
    with open(args.config) as f:
        config = yaml.safe_load(f)
    
    train(args, config)


if __name__ == "__main__":
    main()
